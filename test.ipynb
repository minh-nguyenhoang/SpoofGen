{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "# from time import sleep\n",
    "\n",
    "# for i in (ep_bar:= tqdm(range(4), desc='1st loop')):\n",
    "#     ep_bar.set_description(f'Epoch {i}')\n",
    "#     for j in tqdm(range(5), desc='2nd loop', leave= False):\n",
    "#         for k in tqdm(range(50), desc='3rd loop', leave=False):\n",
    "#             sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw = input(\"Test: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, j in enumerate([1,2,3,4,5,6,7,8]):\n",
    "#     print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spoof_gen.models.backbone.iresnet import iresnet\n",
    "\n",
    "imodel = iresnet('50').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 16, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x131072 and 25088x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ADMIN\\Desktop\\20222\\SpoofGen\\test.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Desktop/20222/SpoofGen/test.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m imodel(torch\u001b[39m.\u001b[39;49mrand((N,\u001b[39m3\u001b[39;49m,\u001b[39m256\u001b[39;49m,\u001b[39m256\u001b[39;49m))\u001b[39m.\u001b[39;49mcuda())\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\20222\\SpoofGen\\spoof_gen\\models\\backbone\\iresnet.py:161\u001b[0m, in \u001b[0;36mIResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    159\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n\u001b[0;32m    160\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m--> 161\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x\u001b[39m.\u001b[39;49mfloat() \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp16 \u001b[39melse\u001b[39;49;00m x)\n\u001b[0;32m    162\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures(x)\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x131072 and 25088x512)"
     ]
    }
   ],
   "source": [
    "imodel(torch.rand((N,3,256,256)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder('resnet-50').backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(layer, name):\n",
    "    return hasattr(layer, \"stride\") and name == \"conv1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_fn(layer):\n",
    "    layer.stride = (1, 1)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patcher.patch_net(model, condition=condition, patch=patch_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model,(3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from spoof_gen.models.backbone.upblock import build_decoder_unet\n",
    "from spoof_gen.models.backbone.mobilenet import mobilenet\n",
    "from spoof_gen.models.backbone.resnet import resnet\n",
    "from spoof_gen.models.backbone.revnet import revnet\n",
    "from spoof_gen.models.generator import Encoder, Decoder, Generator\n",
    "from spoof_gen.models.conditional_generator import ConditionGenerator\n",
    "from spoof_gen.models.critic import Critic\n",
    "# decoder = build_decoder_unet(skip_z= True)\n",
    "\n",
    "N = 1\n",
    "x = torch.rand((N,3,256,256)).cuda()\n",
    "z = torch.rand((N,4,32,32)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def contrast_conv(input : torch.Tensor):\n",
    "    ''' compute contrast depth in both of (out, label) '''\n",
    "    '''\n",
    "        input  Nx3x256x256\n",
    "        output Nx(8x3)x32x32\n",
    "    '''\n",
    "\n",
    "    device = input.device\n",
    "\n",
    "    N,C,H,W = input.shape\n",
    "\n",
    "    input = input.view(N*C,H,W)\n",
    "\n",
    "    kernel_filter_list = [\n",
    "        [[1, 0, 0], [0, -1, 0], [0, 0, 0]], [[0, 1, 0], [0, -1, 0], [0, 0, 0]], [[0, 0, 1], [0, -1, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [1, -1, 0], [0, 0, 0]], [[0, 0, 0], [0, -1, 1], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, -1, 0], [1, 0, 0]], [[0, 0, 0], [0, -1, 0], [0, 1, 0]], [[0, 0, 0], [0, -1, 0], [0, 0, 1]]]\n",
    "\n",
    "    kernel_filter = np.array(kernel_filter_list, float)\n",
    "\n",
    "    kernel_filter = torch.from_numpy(\n",
    "        kernel_filter.astype(np.float)).float().to(device)\n",
    "    # weights (in_channel, out_channel, kernel, kernel)\n",
    "    kernel_filter = kernel_filter.unsqueeze(dim=1)\n",
    "\n",
    "\n",
    "    input = input.unsqueeze(dim=1).expand(\n",
    "        input.shape[0], 8, input.shape[1], input.shape[2])\n",
    "\n",
    "    contrast_img = F.conv2d(\n",
    "        input, weight=kernel_filter, groups=8)  # depthwise conv\n",
    "\n",
    "    contrast_img = contrast_img.view(N,8,C,H-2,W-2)\n",
    "    return contrast_img\n",
    "\n",
    "# Pearson range [-1, 1] so if < 0, abs|loss| ; if >0, 1- loss\n",
    "class Contrast_loss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, out, label):\n",
    "        '''\n",
    "        compute contrast depth in both of (out, label),\n",
    "        then get the loss of them\n",
    "        tf.atrous_convd match tf-versions: 1.4\n",
    "        '''\n",
    "\n",
    "        contrast_out = contrast_conv(out)\n",
    "        \n",
    "        contrast_label = contrast_conv(label)\n",
    "\n",
    "        criterion_MSE = nn.MSELoss()\n",
    "\n",
    "        loss = criterion_MSE(contrast_out, contrast_label)\n",
    "        #loss = torch.pow(contrast_out - contrast_label, 2)\n",
    "        #loss = torch.mean(loss)\n",
    "\n",
    "        return loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spoof_gen.data_utils import StandardDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "train_data = StandardDataset('Data/train_img', transform= None)\n",
    "train_loader = DataLoader(train_data,2)\n",
    "\n",
    "# loss_cdl = Contrast_loss()\n",
    "\n",
    "# idx = np.random.randint(0,len(train_data))\n",
    "# idx2 = np.random.randint(0,len(train_data))\n",
    "\n",
    "# # loss_cdl(train_data[idx][1].unsqueeze(0).unsqueeze(0),train_data[idx2][1].unsqueeze(0).unsqueeze(0))\n",
    "# nn.MSELoss()(train_data[idx][1].unsqueeze(0),train_data[idx2][1].unsqueeze(0))\n",
    "\n",
    "# for img in contrast_conv(train_data[idx][0].unsqueeze(0)).squeeze(0):\n",
    "#     plt.imshow(img.permute(1,2,0).cpu().numpy())\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All states are loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "con_gen = ConditionGenerator(init_from='checkpoints/conditional_generator/conditonal_generator_mobilenetv3_last.pth').cuda()\n",
    "encoder = Encoder('iresnet-50').cuda()\n",
    "decoder = Decoder('revnet-50').cuda()\n",
    "gen = Generator(encoder= encoder, decoder= decoder, con_gen= con_gen)\n",
    "crit = Critic().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, map_x, label, _ = next(iter(train_loader))\n",
    "inputs = inputs.cuda()\n",
    "map_x = map_x.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2951],\n",
       "        [0.1484]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_x = con_gen(inputs,label)\n",
    "crit(inputs,map_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fc.weight',\n",
       " 'fc.bias',\n",
       " 'features.weight',\n",
       " 'features.bias',\n",
       " 'features.running_mean',\n",
       " 'features.running_var']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_pretrain_backbone('checkpoints/backbone_arcface_50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleVAE(nn.Module):\n",
    "    def __init__(self, in_dim = 256) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(in_dim, 128),\n",
    "                                     nn.ReLU(inplace= True),\n",
    "                                     nn.Linear(128, 64),\n",
    "                                     nn.ReLU(inplace= True),\n",
    "                                     nn.Linear(64, 32),\n",
    "                                     nn.ReLU(inplace= True),\n",
    "                                     nn.Linear(32, 16 *2))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "                                     nn.Linear(16, 32),\n",
    "                                     nn.ReLU(inplace= True),\n",
    "                                     nn.Linear(32, 64),\n",
    "                                     nn.ReLU(inplace= True),\n",
    "                                     nn.Linear(64, 128),\n",
    "                                     nn.ReLU(inplace= True),\n",
    "                                     nn.Linear(128, 256))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = torch.split(self.encoder(x), split_size_or_sections= 16, dim= 1)\n",
    "        z = mu + torch.rand_like(log_var) * log_var\n",
    "        out = self.decoder(z)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class SimpleGAN(nn.Module):\n",
    "    def __init__(self, in_dim = 256) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(in_dim, 128),\n",
    "                                 nn.ReLU(inplace= True),\n",
    "                                 nn.Linear(128, 64),\n",
    "                                 nn.ReLU(inplace= True),\n",
    "                                 nn.Linear(64, 32),\n",
    "                                 nn.ReLU(inplace= True),\n",
    "                                 nn.Linear(32, 1),\n",
    "                                 nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def get_grad(self):\n",
    "        grad = []\n",
    "        for param in self.parameters():\n",
    "            grad.append(torch.tensor(param.grad))\n",
    "        self.grad_ = grad\n",
    "        del grad\n",
    "    \n",
    "    def load_grad(self):\n",
    "        for param in self.parameters():\n",
    "            param.grad.copy_(self.grad_.pop(0))\n",
    "\n",
    "        del self.grad_\n",
    "\n",
    "\n",
    "gen = SimpleVAE()\n",
    "dis = SimpleGAN()\n",
    "\n",
    "g_opt = torch.optim.Adam(gen.parameters(),lr = 0.0001)\n",
    "d_opt = torch.optim.Adam(dis.parameters(),lr = 0.0001)\n",
    "\n",
    "x = torch.rand(4,256)\n",
    "out  :torch.Tensor = gen(x)\n",
    "\n",
    "d_real = dis(x)\n",
    "d_gen  = dis(out)\n",
    "loss = (d_gen - d_real).mean()\n",
    "# loss = (((x - out)**2)/2).mean()\n",
    "\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4,256)\n",
    "out  :torch.Tensor = gen(x)\n",
    "\n",
    "d_real = dis(x)\n",
    "d_gen  = dis(out)\n",
    "loss = (d_gen - d_real).mean()\n",
    "# loss = (((x - out)**2)/2).mean()\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.eval()\n",
    "g_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_30856\\3275356399.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  grad.append(torch.tensor(param.grad))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.7181e-04,  5.5979e-04,  7.6311e-04,  ...,  6.3562e-04,\n",
       "          5.5945e-04,  3.5486e-04],\n",
       "        [-8.5978e-05, -2.9133e-05, -2.6214e-05,  ..., -7.6220e-05,\n",
       "         -1.0989e-04, -6.6789e-06],\n",
       "        [-1.2898e-04, -1.3837e-04, -1.3400e-04,  ..., -1.6622e-04,\n",
       "         -1.9605e-04, -8.0626e-05],\n",
       "        ...,\n",
       "        [ 3.9026e-04,  9.3475e-04,  7.9567e-04,  ...,  5.6885e-04,\n",
       "          5.1848e-04,  9.0690e-04],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-3.2699e-04, -3.1702e-04, -1.7325e-04,  ..., -2.2452e-04,\n",
       "         -1.9004e-04, -3.3440e-04]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dis.get_grad()\n",
    "dis.grad_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.7181e-04,  5.5979e-04,  7.6311e-04,  ...,  6.3562e-04,\n",
       "          5.5945e-04,  3.5486e-04],\n",
       "        [-8.5978e-05, -2.9133e-05, -2.6214e-05,  ..., -7.6220e-05,\n",
       "         -1.0989e-04, -6.6789e-06],\n",
       "        [-1.2898e-04, -1.3837e-04, -1.3400e-04,  ..., -1.6622e-04,\n",
       "         -1.9605e-04, -8.0626e-05],\n",
       "        ...,\n",
       "        [ 3.9026e-04,  9.3475e-04,  7.9567e-04,  ...,  5.6885e-04,\n",
       "          5.1848e-04,  9.0690e-04],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-3.2699e-04, -3.1702e-04, -1.7325e-04,  ..., -2.2452e-04,\n",
       "         -1.9004e-04, -3.3440e-04]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis.net[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis.load_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plus_one(a):\n",
    "    a += 1\n",
    "\n",
    "x = 5\n",
    "plus_one(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.backbone._modules['fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    gen_resp = gen(x,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0449]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_x = con_gen(x,1)\n",
    "crit(x,map_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [32] and output size of (256, 256). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ADMIN\\Desktop\\20222\\SpoofGen\\test.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Desktop/20222/SpoofGen/test.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m crit(gen_resp[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m],gen_resp[\u001b[39m1\u001b[39;49m][\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Desktop\\20222\\SpoofGen\\spoof_gen\\models\\critic.py:48\u001b[0m, in \u001b[0;36mCritic.forward\u001b[1;34m(self, x, condition)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x, condition):\n\u001b[0;32m     46\u001b[0m     \u001b[39m# map_x = self.con_gen(x,condition)\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     map_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupsampler(condition\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     49\u001b[0m     \u001b[39mprint\u001b[39m(map_x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     50\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:156\u001b[0m, in \u001b[0;36mUpsample.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49minterpolate(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_factor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malign_corners,\n\u001b[0;32m    157\u001b[0m                          recompute_scale_factor\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecompute_scale_factor)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3866\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   3864\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(size, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m   3865\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m!=\u001b[39m dim:\n\u001b[1;32m-> 3866\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3867\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3868\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput with spatial dimensions of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:])\u001b[39m}\u001b[39;00m\u001b[39m and output size of \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3869\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3870\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3871\u001b[0m \n\u001b[0;32m   3872\u001b[0m         )\n\u001b[0;32m   3873\u001b[0m     output_size \u001b[39m=\u001b[39m size\n\u001b[0;32m   3874\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [32] and output size of (256, 256). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "crit(gen_resp[0][0],gen_resp[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/minh-nguyenhoang/SpoofGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spoof_gen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
